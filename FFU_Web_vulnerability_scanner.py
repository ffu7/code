# -*- coding: utf-8 -*-


from pystyle import *
import validators, webbrowser
import os,argparse, socket, sys
import requests
from urllib.parse import urljoin, urlparse, parse_qs
from datetime import datetime
from bs4 import BeautifulSoup
from ffusubcodes import header,html_result_report, subdomain,  ports_scan,command_injection,xssscanner,sqli,idor,sqlicanner,subdomain_keyword,enumarate_sensible_data

import warnings; warnings.filterwarnings('ignore', message='Unverified HTTPS request')

def print_logo():

    logo ="""

         +-------------------------+
         |    FATİH FURKAN USLU    |
         |  YEDİTEPE ÜNİVERSİTESİ  |
         |    MÜHENDİSLİK PROJESİ  |
         +-------------------------+                           
    """
    
    print(''.join(Colors.red) + logo)
    #Write.Print(f"{logo}", Colors.red_to_green, interval=0.009)
    #Write.Print(f"{logo}", Colors.blue_to_red, interval=0.01)
    #Write.Print(f"{logo}", Colors.red_to_purple, interval=0.01)



      
# get target and  Check given url 
def get_target_url():
    while True:
        print(Colors.red +"https://www.example.com"+Colors.yellow)
        target_url = input("Enter the target URL to scan for vulnerabilities: ")
        validation = validators.url(target_url.strip())
        if validation:
            domain_name = urlparse(target_url).netloc
            try:
                domain_to_ip = socket.gethostbyname(domain_name)
                current_date = datetime.now().strftime('%Y-%m-%d')
                folder_name = f"{domain_name}_{current_date}"
                create_folder(folder_name)
                return target_url, folder_name
            except socket.gaierror:
                print(f"There is no registered domain associated with: {target_url}")
        else:
            print(Colors.red +"Invalid input. Please enter a valid URL.")


def collect_urls_from_file(file_path):
    try:
        with open(file_path, "r") as file:
            websites = file.read().splitlines()
        return websites
    except FileNotFoundError:
        print_error(f"File '{file_path}' not found.")
        return []
# get target sonu

# crawl and save found urls basi

def create_folder(folder_name):
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
        print(Colors.green + f"[*] Folder Created Successfully...{folder_name}")
    else:
        print(Colors.red+ "[!] Target Folder Already Exists...")
        print(Colors.green+ "")
        #exit()

def get_links_from_page(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            links = [urljoin(url, link.get('href')) for link in soup.find_all('a', href=True)]
            return links
        else:
            print(f"Failed to get links from page {url}. Status code: {response.status_code}")
            return []
    except requests.exceptions.RequestException as e:
        print(f"Error (requests) while getting links from page {url}: {e}")
        return []
def remove_duplicate_lines(input_file):
    unique_lines = set()
    with open(input_file, 'r') as file:
        for line in file:
            unique_lines.add(line.strip())
    with open(input_file, 'w') as file:
        for line in unique_lines:
            file.write(line + '\n')
    file.close()
def crawl_all(base_url, domain_name,folder_name,max_depth=3):
    visited_urls = set()
    file=open(folder_name+"/urls.txt", "a")
    file2=open(folder_name+"/non_main_domain_urls.txt", "a")
    def crawl(url, depth):
        try:
            if depth > max_depth or url in visited_urls:
                return

            visited_urls.add(url)
            print(f"Crawling  : {url}")
            links = get_links_from_page(url)
            for link in links:
                print(link)
                if domain_name in link and not any(link.endswith(ext) for ext in [".pdf", ".jpg", ".jpeg", ".png", ".gif"]) :
                    file.write(link + "\n")
                else:
                    file2.write(link + "\n")
                crawl(link, depth + 1)

        except Exception as e:
            print(f"Error in crawl function: {e}")

    crawl(base_url, 1)
    file.close()
    file2.close()

    input_file = folder_name+"/urls.txt" 
    remove_duplicate_lines(input_file)
    input_file = folder_name+"/non_main_domain_urls.txt"
    remove_duplicate_lines(input_file)
# crawl and save results sonu

def scan_single_host_limited(folder_name,base_url):

    domain_name = urlparse(base_url).netloc
   
    # collect data about given url
    shodan.main(folder_name,domain_name)
    #ports_scan.scan(folder_name,domain_name)
    header.hedaersCheck(folder_name,base_url)
    subdomain.getSubdomains(folder_name,domain_name)
    enumarate_log_files.main(folder_name,base_url)
    web_server_metafiles.main(folder_name,base_url)
    
    #find_admin_entry_points.main(folder_name, base_url)

    
    print("Crawling" + base_url)
    crawl_all(base_url, domain_name,folder_name,max_depth=1)
    payloads_file_path = folder_name+"/urls.txt"
    with open(payloads_file_path, "r") as payloads_file:
        crawled_urls = payloads_file.readlines()
        
    # check for some vulnerabilites
    
    for url_to_check in crawled_urls:
        print(f"Testing URL: {url_to_check}\n")
        url_to_check=url_to_check.strip()
        print("Testing sensible data\n")
        enumarate_sensible_data.main(folder_name,url_to_check)
        print("Enumerate Form\n")
        #enumerate_forms.main(folder_name,url_to_check)
        #time.sleep(5)
    html_result_report.generate(folder_name,domain_name, base_url)
    
    try:
        filename = 'file:///'+os.getcwd()+'/' +folder_name+ '/report.html'
        webbrowser.open_new_tab(filename) 
    except FileNotFoundError:
        print(f"Error: File '{filename}' not found.")
    
    
def scan_single_host(folder_name,base_url):

    domain_name = urlparse(base_url).netloc

    # collect data about given url
    ports_scan.scan(folder_name,domain_name)
    header.hedaersCheck(folder_name,base_url)
    subdomain.getSubdomains(folder_name,domain_name)
    subdomain_keyword.getSubdomains(folder_name,base_url)

    
    #crawl 
    
    print("Crawling" + base_url)
    crawl_all(base_url, domain_name,folder_name,max_depth=1)
    
    payloads_file_path = folder_name+"/urls.txt"
    with open(payloads_file_path, "r") as payloads_file:
        crawled_urls = payloads_file.readlines()
        
    # check for some vulnerabilites
    
    for url_to_check in crawled_urls:
        print(f"Testing URL: {url_to_check}\n")
        url_to_check=url_to_check.strip()

        print("Testing SQLI\n")

        sqli.error_based(folder_name,url_to_check)
        #sqli.or_based(folder_name,url_to_check)
        sqli.union_based(folder_name,url_to_check)
        sqlicanner.main(folder_name,url_to_check)

        print("Testing XSS\n")
        xssscanner.main(folder_name,url_to_check)

        print("Testing command injection XSS\n")
        command_injection.main(folder_name,url_to_check)

        print("Testing sensible data\n")
        enumarate_sensible_data.main(folder_name,url_to_check)


        print(f"Testing idor for  : {url_to_check}\n")
        idor.main(folder_name,url_to_check)
        print("##############################################\n")

    html_result_report.generate(folder_name,domain_name, base_url)

    try:
        filename = 'file:///'+os.getcwd()+'/' +folder_name+ '/report.html'
        webbrowser.open_new_tab(filename) 
    except FileNotFoundError:
        print(f"Error: File '{filename}' not found.")
    
def show_menu():

    print(Colors.green+ "")
    print("Menu:")
    print(Colors.red+"1.Scan Single target for :")
    print(Colors.green+"  1.1. Crawl                 1.2.Check sensitive information")
    print("  1.3. Check for some ports  1.4. Check Header Security")
    print("  1.5. Enumarate sub domain  1.6. SQL injection   ")
    print("  1.6. XSS                   1.7.IDOR - Insecure direct object references ")


    print(Colors.white+"\nCheck for spesific URL (http://sample.com/user_login.php):")
    print(Colors.red+"2. SQL injection -> union based")
    print("3. SQL injection -> error based")
    print("4. SQL injection -> with list of payloads")
    print("5. XSS")
    print("6. Comand injection")
    
    print("0. Exit")
    print(Colors.green+ "")
def main():
    while True:
        os.system("cls")
        print_logo()
        show_menu()
        choice = input("Enter your choice : ")

        if choice == '1':
            user_choice = input("Choose an option:\n1. Enter the target URL to scan for vulnerabilities\n2."
            " Load a list of websites from a txt file\nEnter your choice (1 or 2): " )

            if user_choice == "1":
                target_url, folder_name = get_target_url()
                scan_single_host(folder_name,target_url)
            elif user_choice == "2":
                file_path = input("Enter the path of the txt file containing the list of websites: ")
                websites = collect_urls_from_file(file_path)
                if not websites:
                    print("No websites loaded from the file.")
                    continue
                else:
                    print(f"Loaded {len(websites)} websites from the file.")
        

        elif choice == '2':
            target_url, folder_name = get_target_url()
            sqli.union_based(folder_name,target_url)
        elif choice == '3':
            target_url, folder_name = get_target_url()
            sqli.error_based(folder_name,target_url)
        elif choice == '4':
            target_url, folder_name = get_target_url()
            sqlicanner.main(folder_name,target_url)
        elif choice == '5':
            target_url, folder_name = get_target_url()
            xssscanner.main(folder_name,target_url)
        elif choice == '6':
            target_url, folder_name = get_target_url()
            command_injection.main(folder_name,target_url)

        elif choice == '0':
            print("Exiting...")
            break
        else:
            print("Invalid choice. Please enter a valid option.")
        print(Colors.green+ "\nPress Enter to continue")
        input()
if __name__ == "__main__":
    main()
